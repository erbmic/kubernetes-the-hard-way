- name: Create K8s Hard-Way VMs on libvirt (Ansible-only)
  hosts: localhost
  gather_facts: false
  vars_files: ['../vars/vms.yml']
  collections: [community.libvirt]
  vars:
    ssh_key: "{{ lookup('file', lookup('env','HOME') + '/.ssh/id_rsa.pub') }}"
    libvirt_pool: default
    libvirt_uri: "qemu:///system"
    pool_dir: "/var/lib/libvirt/images"

  tasks: 
    - name: Ensure pool directory exists
      become: true
      file:
        path: "{{ pool_dir }}"
        state: directory
        mode: "0755"

    - name: List existing pools (system URI)
      community.libvirt.virt_pool:
        command: list_pools
        uri: "{{ libvirt_uri }}"
      register: pools

    - name: Define dir pool if missing (idempotent)
      community.libvirt.virt_pool:
        uri: "{{ libvirt_uri }}"
        command: define
        name: "{{ libvirt_pool }}"
        xml: |
          <pool type='dir'>
            <name>{{ libvirt_pool }}</name>
            <target><path>{{ pool_dir }}</path></target>
          </pool>
      when: libvirt_pool not in (pools.pools | default([]))

    - name: Ensure pool is active
      community.libvirt.virt_pool:
        uri: "{{ libvirt_uri }}"
        name: "{{ libvirt_pool }}"
        state: active

    - name: Ensure pool autostart is enabled (via virsh)
      command: "virsh -c {{ libvirt_uri }} pool-autostart {{ libvirt_pool }}"
      register: autostart_cmd
      changed_when: autostart_cmd.rc == 0
      failed_when: autostart_cmd.rc != 0 and
                  ('already active' not in (autostart_cmd.stderr | default(''))) and
                  ('is already marked as autostarted' not in (autostart_cmd.stderr | default('')))

    - name: Ensure cloud image present
      get_url:
        url: "{{ image_url }}"
        dest: "../images/jammy.qcow2"
        mode: "0644"

    - name: Refresh storage pool
      community.libvirt.virt_pool:
        name: "{{ libvirt_pool }}"
        command: refresh

    - name: Get pool XML
      command: "virsh -c {{ libvirt_uri }} pool-dumpxml {{ libvirt_pool }}"
      register: pool_xml
      changed_when: false

    - name: Extract pool path
      set_fact:
        pool_path: "{{ pool_xml.stdout | regex_search('<path>(.*?)</path>', '\\1') }}"

    - name: Create qcow2 volumes with backing (libvirt XML)
      community.libvirt.virt_volume:
        pool: "{{ libvirt_pool }}"
        state: present
        xml: |
          <volume type='file'>
            <name>{{ item.name }}.qcow2</name>
            <capacity unit='G'>{{ item.disk_gb }}</capacity>
            <target>
              <format type='qcow2'/>
            </target>
            <backingStore type='file'>
              <path>{{ (playbook_dir + '/../images/jammy.qcow2') | realpath }}</path>
              <format type='qcow2'/>
            </backingStore>
          </volume>
      loop: "{{ vms }}"

    - name: Render cloud-init user-data
      template:
        src: "../cloudinit/user-data.tpl"
        dest: "../cloudinit/{{ item.name }}-user-data"
      loop: "{{ vms }}"
      vars:
        name: "{{ item.name }}"
        ssh_key: "{{ ssh_key | default('') }}"

    - name: Render cloud-init network-config
      template:
        src: "../cloudinit/network-config.tpl"
        dest: "../cloudinit/{{ item.name }}-network-config"
      loop: "{{ vms }}"
      vars:
        ip: "{{ item.ip }}"
        gw: "{{ item.gw }}"
        dns: "{{ item.dns }}"

    - name: Create CIDATA CDROM via libvirt
      community.libvirt.virt_volume:
        pool: "{{ libvirt_pool }}"
        command: create_cidata_cdrom
        name: "{{ item.name }}-seed.iso"
        cloudinit_config:
          USERDATA: "{{ lookup('file', playbook_dir + '/../cloudinit/' + item.name + '-user-data') }}"
          NETWORK_CONFIG: "{{ lookup('file', playbook_dir + '/../cloudinit/' + item.name + '-network-config') }}"
      loop: "{{ vms }}"

    - name: Normalize pool_path (strip brackets just in case)
      set_fact:
        pool_path: "{{ pool_path | regex_replace('^\\[', '') | regex_replace('\\]$', '') }}"

    - name: Check if domain exists
      command: "virsh -c {{ libvirt_uri }} dominfo {{ item.name }}"
      register: dominfo
      changed_when: false
      failed_when: false
      loop: "{{ vms }}"

    - name: List existing networks (system URI)
      community.libvirt.virt_net:
        command: list_nets
        uri: "{{ libvirt_uri }}"
      register: nets

    - name: Define vagrant-libvirt network if missing (192.168.121.0/24 on virbr1)
      community.libvirt.virt_net:
        uri: "{{ libvirt_uri }}"
        command: define
        name: "{{ libvirt_network }}"
        xml: |
          <network>
            <name>{{ libvirt_network }}</name>
            <forward mode='nat'/>
            <bridge name='virbr1' stp='on' delay='0'/>
            <ip address='192.168.121.1' netmask='255.255.255.0'>
              <dhcp>
                <range start='192.168.121.2' end='192.168.121.254'/>
              </dhcp>
            </ip>
          </network>
      when: libvirt_network not in (nets.networks | default([]))

    - name: Ensure network is active
      community.libvirt.virt_net:
        uri: "{{ libvirt_uri }}"
        name: "{{ libvirt_network }}"
        state: active

    - name: Ensure network autostarts on boot
      command: "virsh -c {{ libvirt_uri }} net-autostart {{ libvirt_network }}"
      register: net_autostart
      changed_when: net_autostart.rc == 0
      failed_when: net_autostart.rc != 0 and
                  ('already' not in (net_autostart.stderr | default('')))

    # 2) Domain anlegen, falls sie fehlt (Index aus vorherigem Task referenzieren)
    - name: Create domain via virt-install (import existing disk)
      command: >
        virt-install
          --connect {{ libvirt_uri }}
          --name {{ item.name }}
          --memory {{ item.mem_mb }}
          --vcpus {{ item.vcpus }}
          --import
          --disk path={{ pool_path }}/{{ item.name }}.qcow2,format=qcow2,bus=virtio
          --disk path={{ pool_path }}/{{ item.name }}-seed.iso,device=cdrom,bus=sata,readonly=on
          --network network={{ libvirt_network }},model=virtio
          --graphics none
          --os-variant ubuntu22.04
          --noautoconsole
      args:
        creates: "/etc/libvirt/qemu/{{ item.name }}.xml"
      loop: "{{ vms }}"
      loop_control:
        index_var: vm_idx            # <<— WICHTIG: eigenen Indexnamen setzen
      when: dominfo.results[vm_idx].rc != 0
      changed_when: true

    # 3) Autostart setzen (tolerant)
    - name: Enable autostart
      command: "virsh -c {{ libvirt_uri }} autostart {{ item.name }}"
      register: autostart_res
      changed_when: autostart_res.rc == 0
      failed_when: autostart_res.rc != 0 and
                  ('already' not in (autostart_res.stderr | default('')))
      loop: "{{ vms }}"

    # 4) Sicherstellen, dass die Domain läuft (idempotent)
    - name: Ensure domain running
      command: "virsh -c {{ libvirt_uri }} start {{ item.name }}"
      register: start_res
      changed_when: start_res.rc == 0
      failed_when: start_res.rc != 0 and
                  ('is already running' not in (start_res.stderr | default('')))
      loop: "{{ vms }}"

    - name: Wait for SSH on each VM
      wait_for:
        host: "{{ item.ip.split('/')[0] }}"
        port: 22
        timeout: 300
      loop: "{{ vms }}"
